{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analise_liar_dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnr6KJBF+dR3T0anocaVDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagorainmaker77/paper/blob/master/analise_liar_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pyzcUeKpV-"
      },
      "source": [
        "!pip install zeugma\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHiTnQocKupD"
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn as skl\n",
        "import seaborn as sb\n",
        "import re\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from zeugma.embeddings import EmbeddingTransformer\n",
        "\n",
        "import warnings\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfKELBRZNk1o",
        "outputId": "83c4fd32-9c4d-41c8-ad54-1106463a8db3"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available :\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnFBBI5VLugz"
      },
      "source": [
        "#Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aql4OT0CLqIA"
      },
      "source": [
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "\n",
        "class Thiago_Mestrado_Oracle(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rng = 42\n",
        "\n",
        "    def get_coefficients(self, y_true, y_pred_a, y_pred_b):\n",
        "        a, b, c, d = 0, 0, 0, 0\n",
        "        for i in range(y_true.shape[0]):\n",
        "            if (y_pred_a[i] == y_true[i]) & (y_pred_b[i] == y_true[i]):\n",
        "                a = a + 1\n",
        "            elif (y_pred_a[i] != y_true[i]) & (y_pred_b[i] == y_true[i]):\n",
        "                b = b + 1\n",
        "            elif (y_pred_a[i] == y_true[i]) & (y_pred_b[i] != y_true[i]):\n",
        "                c = c + 1\n",
        "            else:\n",
        "                d = d + 1\n",
        "\n",
        "        return a, b, c, d\n",
        "\n",
        "    def double_fault_measure(self, y_true, y_pred_a, y_pred_b):\n",
        "        a, b, c, d = self.get_coefficients(y_true, y_pred_a, y_pred_b)\n",
        "        return float(d) / (a + b + c + d)\n",
        "\n",
        "\n",
        "\n",
        "    def monta_oracle(self, label, pre={}, sz_w=5, sz_h=5, titulo=''):\n",
        "\n",
        "        oracle = []\n",
        "        grafico = []\n",
        "\n",
        "        for i in range(len(label)):\n",
        "            oracle.append(0)\n",
        "\n",
        "        for i in range(len(pre)):\n",
        "            grafico.append(0)\n",
        "\n",
        "        for x in pre:\n",
        "            i = 0\n",
        "            for y in pre[x]:\n",
        "                if (label[i] == y):\n",
        "                    oracle[i] = oracle[i] + 1\n",
        "                i = i + 1\n",
        "\n",
        "        erros = 0;\n",
        "        acertos = 0;\n",
        "        for x in oracle:\n",
        "            if (x == 0):\n",
        "                erros = erros + 1\n",
        "            else:\n",
        "                acertos = acertos + 1\n",
        "\n",
        "        barras = {}\n",
        "        for x in oracle:\n",
        "            if (x == 0):\n",
        "                continue\n",
        "            grafico[x - 1] = grafico[x - 1] + 1\n",
        "\n",
        "        label = []\n",
        "        k = 1\n",
        "        for x in grafico:\n",
        "            label.append(k)\n",
        "            k = k + 1\n",
        "        index = np.arange(len(label))\n",
        "        plt.figure(figsize=(sz_w, sz_h))\n",
        "        plt.bar(index, grafico)\n",
        "        plt.xlabel('Acerto por classificador', fontsize=10)\n",
        "        plt.xticks(index, label, fontsize=10)\n",
        "        plt.title(titulo+'total de acertos em conjunto:' + str(acertos))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def monta_tsne(self, classificadores, statement, label, title='', predicoes={}, escala=500, size=5, x_ini=0, x_fim=0, y_ini=0, y_fim=0):\n",
        "\n",
        "\n",
        "\n",
        "        predicoes_test = {}\n",
        "        for x in classificadores:\n",
        "            predicoes[x] = classificadores[x].predict(statement)\n",
        "\n",
        "        m = len(predicoes)\n",
        "        n = len(predicoes)\n",
        "\n",
        "        delta = [[0 for x in range(n)] for x in range(m)]\n",
        "        labels = []\n",
        "\n",
        "        i = 0\n",
        "        for x in predicoes:\n",
        "            k = 0\n",
        "            labels.append(x)\n",
        "            for y in predicoes:\n",
        "                delta[i][k] = self.double_fault_measure(label.array, predicoes[x], predicoes[y])\n",
        "                k = k + 1\n",
        "            i = i + 1\n",
        "\n",
        "        delta = np.array(delta)\n",
        "\n",
        "        tsne_model = TSNE(perplexity=50, init='pca', n_iter=2500, random_state=23)\n",
        "        new_values = tsne_model.fit_transform(delta)\n",
        "\n",
        "        x = []\n",
        "        y = []\n",
        "        for value in new_values:\n",
        "            x.append(value[0])\n",
        "            y.append(value[1])\n",
        "\n",
        "        plt.figure(figsize=(size, size))\n",
        "        plt.title(title)\n",
        "\n",
        "        if escala > 0:\n",
        "          plt.ylim(escala * -1, escala)\n",
        "          plt.xlim(escala * -1, escala)\n",
        "        else:\n",
        "          plt.ylim(y_ini, y_fim)\n",
        "          plt.xlim(x_ini, x_fim)\n",
        "\n",
        "        simbolos = {}\n",
        "        simbolos[0] = 'X'\n",
        "        simbolos[1] = 'd'\n",
        "        simbolos[2] = '*'\n",
        "        simbolos[3] = \"^\"\n",
        "        simbolos[4] = 'o'\n",
        "        \n",
        "        dot = 0\n",
        "        for i in range(len(x)):\n",
        "            plt.scatter(x[i], y[i], marker=simbolos[dot], c='#808080', s=100)\n",
        "            print(labels[i], simbolos[dot])\n",
        "            #plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='center', va='bottom')\n",
        "            dot = dot+1\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def monta_tsne_full(self, classificadores, statement, label, title='', predicoes={}, escala=500, size=5, x_ini=0, x_fim=0, y_ini=0, y_fim=0):\n",
        "\n",
        "\n",
        "\n",
        "        predicoes_test = {}\n",
        "        for x in classificadores:\n",
        "            predicoes[x] = classificadores[x].predict(statement)\n",
        "\n",
        "        m = len(predicoes)\n",
        "        n = len(predicoes)\n",
        "\n",
        "        delta = [[0 for x in range(n)] for x in range(m)]\n",
        "        labels = []\n",
        "\n",
        "        i = 0\n",
        "        for x in predicoes:\n",
        "            k = 0\n",
        "            labels.append(x)\n",
        "            for y in predicoes:\n",
        "                delta[i][k] = self.double_fault_measure(label.array, predicoes[x], predicoes[y])\n",
        "                k = k + 1\n",
        "            i = i + 1\n",
        "\n",
        "        delta = np.array(delta)\n",
        "\n",
        "        tsne_model = TSNE(perplexity=50, init='pca', n_iter=2500, random_state=23)\n",
        "        new_values = tsne_model.fit_transform(delta)\n",
        "\n",
        "        x = []\n",
        "        y = []\n",
        "        for value in new_values:\n",
        "            x.append(value[0])\n",
        "            y.append(value[1])\n",
        "\n",
        "        plt.figure(figsize=(size, size))\n",
        "        plt.title(title)\n",
        "\n",
        "        if escala > 0:\n",
        "          plt.ylim(escala * -1, escala)\n",
        "          plt.xlim(escala * -1, escala)\n",
        "        else:\n",
        "          plt.ylim(y_ini, y_fim)\n",
        "          plt.xlim(x_ini, x_fim)\n",
        "\n",
        "        simbolos = {}\n",
        "        simbolos[0] = 'X'\n",
        "        simbolos[1] = 'd'\n",
        "        simbolos[2] = '*'\n",
        "        simbolos[3] = \"^\"\n",
        "        simbolos[4] = 'o'\n",
        "        \n",
        "        dot = 0\n",
        "        for i in range(len(x)):\n",
        "            plt.scatter(x[i], y[i] , c='#808080', s=10)\n",
        "            plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='center', va='bottom')\n",
        "            dot = dot+1\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def monta_tsne_pred(self,statement, label, title='', predicoes={}, escala=500, size=5, x_ini=0, x_fim=0, y_ini=0, y_fim=0):\n",
        "\n",
        "\n",
        "        m = len(predicoes)\n",
        "        n = len(predicoes)\n",
        "\n",
        "        delta = [[0 for x in range(n)] for x in range(m)]\n",
        "        labels = []\n",
        "\n",
        "        i = 0\n",
        "        for x in predicoes:\n",
        "            k = 0\n",
        "            labels.append(x)\n",
        "            for y in predicoes:\n",
        "                delta[i][k] = self.double_fault_measure(label.array, predicoes[x], predicoes[y])\n",
        "                k = k + 1\n",
        "            i = i + 1\n",
        "\n",
        "        delta = np.array(delta)\n",
        "\n",
        "        tsne_model = TSNE(perplexity=50, init='pca', n_iter=2500, random_state=23)\n",
        "        new_values = tsne_model.fit_transform(delta)\n",
        "\n",
        "        x = []\n",
        "        y = []\n",
        "        for value in new_values:\n",
        "            x.append(value[0])\n",
        "            y.append(value[1])\n",
        "\n",
        "        plt.figure(figsize=(size, size))\n",
        "        plt.title(title)\n",
        "\n",
        "        if escala > 0:\n",
        "          plt.ylim(escala * -1, escala)\n",
        "          plt.xlim(escala * -1, escala)\n",
        "        else:\n",
        "          plt.ylim(y_ini, y_fim)\n",
        "          plt.xlim(x_ini, x_fim)\n",
        "\n",
        "        simbolos = {}\n",
        "        simbolos[0] = 'X'\n",
        "        simbolos[1] = 'd'\n",
        "        simbolos[2] = '*'\n",
        "        simbolos[3] = \"^\"\n",
        "        simbolos[4] = 'o'\n",
        "        \n",
        "        dot = 0\n",
        "        for i in range(len(x)):\n",
        "            plt.scatter(x[i], y[i], marker=simbolos[dot], c='#808080', s=100)\n",
        "            print(labels[i], simbolos[dot])\n",
        "            #plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='center', va='bottom')\n",
        "            dot = dot+1\n",
        "        plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzGLj684L_GW"
      },
      "source": [
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# teste\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(next(iter(word2vec.items())))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "\n",
        "\n",
        "class Mestrado_Thiago_util(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rng = 42\n",
        "\n",
        "    def get_dataset(self):\n",
        "        uri_train = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv'\n",
        "        uri_valid = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/valid.tsv'\n",
        "        uri_test = 'https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv'\n",
        "        df_train = pd.read_table(uri_train,\n",
        "                                 names=['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
        "                                        'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c', 'pants_on_fire_c',\n",
        "                                        'venue'])\n",
        "        df_valid = pd.read_table(uri_valid,\n",
        "                                 names=['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
        "                                        'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c', 'pants_on_fire_c',\n",
        "                                        'venue'])\n",
        "        df_test = pd.read_csv(uri_test, sep='\\t',\n",
        "                              names=['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
        "                                     'barely_true_c', 'false_c', 'half_true_c', 'mostly_true_c', 'pants_on_fire_c',\n",
        "                                     'venue'])\n",
        "        df = pd.concat([df_train, df_valid])\n",
        "\n",
        "        return df_train, df_valid, df_test, df\n",
        "\n",
        "    def get_coefficients(self, y_true, y_pred_a, y_pred_b):\n",
        "        a, b, c, d = 0, 0, 0, 0\n",
        "        for i in range(y_true.shape[0]):\n",
        "            if (y_pred_a[i] == y_true[i]) & (y_pred_b[i] == y_true[i]):\n",
        "                a = a + 1\n",
        "            elif (y_pred_a[i] != y_true[i]) & (y_pred_b[i] == y_true[i]):\n",
        "                b = b + 1\n",
        "            elif (y_pred_a[i] == y_true[i]) & (y_pred_b[i] != y_true[i]):\n",
        "                c = c + 1\n",
        "            else:\n",
        "                d = d + 1\n",
        "\n",
        "        return a, b, c, d\n",
        "\n",
        "    def double_fault_measure(self, y_true, y_pred_a, y_pred_b):\n",
        "        a, b, c, d = get_coefficients(y_true, y_pred_a, y_pred_b)\n",
        "        return float(d) / (a + b + c + d)\n",
        "\n",
        "    def monta_oracle(self, label, predicoes={}, sz=5):\n",
        "\n",
        "        oracle = []\n",
        "        grafico = []\n",
        "\n",
        "        for i in range(len(label)):\n",
        "            oracle.append(0)\n",
        "\n",
        "        for i in range(len(predicoes)):\n",
        "            grafico.append(0)\n",
        "\n",
        "        for x in predicoes:\n",
        "            i = 0\n",
        "            for y in predicoes[x]:\n",
        "                if (label[i] == y):\n",
        "                    oracle[i] = oracle[i] + 1\n",
        "                i = i + 1\n",
        "\n",
        "        erros = 0;\n",
        "        acertos = 0;\n",
        "        for x in oracle:\n",
        "            if (x == 0):\n",
        "                erros = erros + 1\n",
        "            else:\n",
        "                acertos = acertos + 1\n",
        "\n",
        "        barras = {}\n",
        "        for x in oracle:\n",
        "            if (x == 0):\n",
        "                continue\n",
        "            grafico[x - 1] = grafico[x - 1] + 1\n",
        "\n",
        "        label = []\n",
        "        k = 1\n",
        "        for x in grafico:\n",
        "            label.append(k)\n",
        "            k = k + 1\n",
        "        index = np.arange(len(label))\n",
        "        # plt.figure(figsize=(sz+3, sz))\n",
        "        plt.bar(index, grafico)\n",
        "        plt.xlabel('Acerto por classificador', fontsize=10)\n",
        "        plt.xticks(index, label, fontsize=10)\n",
        "        plt.title('Total de acertos em conjunto:' + str(acertos))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#https://github.com/thiagorainmaker77/libs.git\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_perceptron(self, statement, label, glove, w2v, fasttext):\n",
        "        lr1 = Perceptron(random_state=42)\n",
        "\n",
        "        lr2 = Perceptron(random_state=42)\n",
        "\n",
        "        lr3 = Perceptron(random_state=42)\n",
        "\n",
        "        lr4 = Perceptron(random_state=42)\n",
        "\n",
        "        lr5 = Perceptron(random_state=42)\n",
        "\n",
        "        countV = CountVectorizer()\n",
        "        countV.fit_transform(statement)\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        lr_cv = Pipeline([\n",
        "            ('NBCV', countV),\n",
        "            ('clf', lr1)])\n",
        "\n",
        "        lr_tfid = Pipeline([\n",
        "            ('NBCV', tfidf_ngram),\n",
        "            ('clf', lr2)])\n",
        "\n",
        "        lr_w2v = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('clf', lr3)])\n",
        "\n",
        "        lr_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('clf', lr4)])\n",
        "\n",
        "        lr_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('clf', lr5)])\n",
        "\n",
        "        lr_cv.fit(statement, label)\n",
        "        lr_tfid.fit(statement, label)\n",
        "        lr_w2v.fit(statement, label)\n",
        "        lr_glove.fit(statement, label)\n",
        "        lr_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = lr_cv\n",
        "        classificadores[\"TFIDF\"] = lr_tfid\n",
        "        classificadores[\"Word2Vec\"] = lr_w2v\n",
        "        classificadores[\"Glove\"] = lr_glove\n",
        "        classificadores[\"FastText\"] = lr_fast\n",
        "        return classificadores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_pac(self, statement, label, glove, w2v, fasttext):\n",
        "        lr1 = PassiveAggressiveClassifier(random_state=42)\n",
        "\n",
        "        lr2 = PassiveAggressiveClassifier(random_state=42)\n",
        "\n",
        "        lr3 = PassiveAggressiveClassifier(random_state=42)\n",
        "\n",
        "        lr4 = PassiveAggressiveClassifier(random_state=42)\n",
        "\n",
        "        lr5 = PassiveAggressiveClassifier(random_state=42)\n",
        "\n",
        "        countV = CountVectorizer()\n",
        "        countV.fit_transform(statement)\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        lr_cv = Pipeline([\n",
        "            ('NBCV', countV),\n",
        "            ('clf', lr1)])\n",
        "\n",
        "        lr_tfid = Pipeline([\n",
        "            ('NBCV', tfidf_ngram),\n",
        "            ('clf', lr2)])\n",
        "\n",
        "        lr_w2v = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('clf', lr3)])\n",
        "\n",
        "        lr_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('clf', lr4)])\n",
        "\n",
        "        lr_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('clf', lr5)])\n",
        "\n",
        "        lr_cv.fit(statement, label)\n",
        "        lr_tfid.fit(statement, label)\n",
        "        lr_w2v.fit(statement, label)\n",
        "        lr_glove.fit(statement, label)\n",
        "        lr_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = lr_cv\n",
        "        classificadores[\"TFIDF\"] = lr_tfid\n",
        "        classificadores[\"Word2Vec\"] = lr_w2v\n",
        "        classificadores[\"Glove\"] = lr_glove\n",
        "        classificadores[\"FastText\"] = lr_fast\n",
        "        return classificadores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_knn(self, statement, label, glove, w2v, fasttext):\n",
        "        lr1 = KNeighborsClassifier()\n",
        "\n",
        "        lr2 = KNeighborsClassifier()\n",
        "\n",
        "        lr3 = KNeighborsClassifier()\n",
        "\n",
        "        lr4 = KNeighborsClassifier()\n",
        "\n",
        "        lr5 = KNeighborsClassifier()\n",
        "\n",
        "        countV = CountVectorizer()\n",
        "        countV.fit_transform(statement)\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        lr_cv = Pipeline([\n",
        "            ('NBCV', countV),\n",
        "            ('clf', lr1)])\n",
        "\n",
        "        lr_tfid = Pipeline([\n",
        "            ('NBCV', tfidf_ngram),\n",
        "            ('clf', lr2)])\n",
        "\n",
        "        lr_w2v = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('clf', lr3)])\n",
        "\n",
        "        lr_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('clf', lr4)])\n",
        "\n",
        "        lr_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('clf', lr5)])\n",
        "\n",
        "        lr_cv.fit(statement, label)\n",
        "        lr_tfid.fit(statement, label)\n",
        "        lr_w2v.fit(statement, label)\n",
        "        lr_glove.fit(statement, label)\n",
        "        lr_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = lr_cv\n",
        "        classificadores[\"TFIDF\"] = lr_tfid\n",
        "        classificadores[\"Word2Vec\"] = lr_w2v\n",
        "        classificadores[\"Glove\"] = lr_glove\n",
        "        classificadores[\"FastText\"] = lr_fast\n",
        "        return classificadores\n",
        "\n",
        "    def get_lr(self, statement, label, glove, w2v, fasttext):\n",
        "\n",
        "        lr1 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                 intercept_scaling=1, max_iter=100, \n",
        "                                 n_jobs=None, penalty='l2',\n",
        "                                 random_state=self.rng, tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "        lr2 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                 intercept_scaling=1, max_iter=100, \n",
        "                                 n_jobs=None, penalty='l2',\n",
        "                                 random_state=self.rng, tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "        lr3 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                 intercept_scaling=1, max_iter=100,\n",
        "                                 n_jobs=None, penalty='l2',\n",
        "                                 random_state=self.rng, tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "        lr4 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                 intercept_scaling=1, max_iter=100, \n",
        "                                 n_jobs=None, penalty='l2',\n",
        "                                 random_state=self.rng, tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "        lr5 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                 intercept_scaling=1, max_iter=100, \n",
        "                                 n_jobs=None, penalty='l2',\n",
        "                                 random_state=self.rng, tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "        countV = CountVectorizer(analyzer='word', binary=False, decode_error='strict', encoding='utf-8',\n",
        "                                 input='content',\n",
        "                                 lowercase=False, max_df=0.5, max_features=None, min_df=1,\n",
        "                                 ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "                                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                                 tokenizer=None, vocabulary=None)\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        lr_cv = Pipeline([\n",
        "            ('NBCV', countV),\n",
        "            ('clf', lr1)])\n",
        "\n",
        "        lr_tfid = Pipeline([\n",
        "            ('NBCV', tfidf_ngram),\n",
        "            ('clf', lr2)])\n",
        "\n",
        "        lr_w2v = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('clf', lr3)])\n",
        "\n",
        "        lr_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('clf', lr4)])\n",
        "\n",
        "        lr_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('clf', lr5)])\n",
        "\n",
        "        lr_cv.fit(statement, label)\n",
        "        lr_tfid.fit(statement, label)\n",
        "        lr_w2v.fit(statement, label)\n",
        "        lr_glove.fit(statement, label)\n",
        "        lr_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = lr_cv\n",
        "        classificadores[\"TFIDF\"] = lr_tfid\n",
        "        classificadores[\"Word2Vec\"] = lr_w2v\n",
        "        classificadores[\"Glove\"] = lr_glove\n",
        "        classificadores[\"FastText\"] = lr_fast\n",
        "        return classificadores\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def get_sgd(self, statement, label, glove, w2v, fasttext):\n",
        "\n",
        "        countV = CountVectorizer(analyzer='word', binary=False, decode_error='strict', encoding='utf-8',\n",
        "                                 input='content',\n",
        "                                 lowercase=False, max_df=0.5, max_features=None, min_df=1,\n",
        "                                 ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "                                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                                 tokenizer=None, vocabulary=None)\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        sgd_cv = Pipeline([\n",
        "            ('sgd_tfidf', countV),\n",
        "            ('sgd_clf', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        sgd_tf = Pipeline([\n",
        "            ('sgd_tfidf', tfidf_ngram),\n",
        "            ('sgd_clf', SGDClassifier(loss='log', penalty='l2', random_state=self.rng, alpha=1e-3))\n",
        "        ])\n",
        "\n",
        "        sgd_w2v = Pipeline([\n",
        "            ('sgd_tfidf', w2v),\n",
        "            ('sgd_clf', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        sgd_glove = Pipeline([\n",
        "            ('sgd_tfidf', glove),\n",
        "            ('sgd_clf', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        sgd_fast = Pipeline([\n",
        "            ('sgd_tfidf', fasttext),\n",
        "            ('sgd_clf', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        sgd_cv.fit(statement, label)\n",
        "        sgd_tf.fit(statement, label)\n",
        "        sgd_w2v.fit(statement, label)\n",
        "        sgd_glove.fit(statement, label)\n",
        "        sgd_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = sgd_cv\n",
        "        classificadores[\"TFIDF\"] = sgd_tf\n",
        "        classificadores[\"Word2Vec\"] = sgd_w2v\n",
        "        classificadores[\"Glove\"] = sgd_glove\n",
        "        classificadores[\"FastText\"] = sgd_fast\n",
        "\n",
        "        return classificadores\n",
        "\n",
        "    def get_svm(self, statement, label, glove, w2v, fasttext):\n",
        "\n",
        "        countV = CountVectorizer(stop_words='english')\n",
        "        countV.fit_transform(statement)\n",
        "\n",
        "        svm.SVC\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        svm_cv = Pipeline([\n",
        "            ('svmCV', countV),\n",
        "            ('svm_clf', svm.SVC(probability=True, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_tf = Pipeline([\n",
        "            ('svmCV', tfidf_ngram),\n",
        "            ('svm_clf', svm.SVC(probability=True, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_w2v = Pipeline([\n",
        "            ('sgd_tfidf', w2v),\n",
        "            ('sgd_clf', svm.SVC(probability=True, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_glove = Pipeline([\n",
        "            ('sgd_tfidf', glove),\n",
        "            ('sgd_clf', svm.SVC(probability=True, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_fast = Pipeline([\n",
        "            ('sgd_tfidf', fasttext),\n",
        "            ('sgd_clf', svm.SVC(probability=True, random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_cv.fit(statement, label)\n",
        "        svm_tf.fit(statement, label)\n",
        "        svm_w2v.fit(statement, label)\n",
        "        svm_glove.fit(statement, label)\n",
        "        svm_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = svm_cv\n",
        "        classificadores[\"TFIDF\"] = svm_tf\n",
        "        classificadores[\"Word2Vec\"] = svm_w2v\n",
        "        classificadores[\"Glove\"] = svm_glove\n",
        "        classificadores[\"FastText\"] = svm_fast\n",
        "\n",
        "        return classificadores\n",
        "\n",
        "\n",
        "\n",
        "    def get_svm_linear_svc(self, statement, label, glove, w2v, fasttext):\n",
        "\n",
        "        countV = CountVectorizer(stop_words='english')\n",
        "        countV.fit_transform(statement)\n",
        "\n",
        "        svm.SVC\n",
        "\n",
        "        tfidf_ngram = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_ngram.fit_transform(statement)\n",
        "\n",
        "        svm_cv = Pipeline([\n",
        "            ('svmCV', countV),\n",
        "            ('svm_clf', svm.LinearSVC(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_tf = Pipeline([\n",
        "            ('svmCV', tfidf_ngram),\n",
        "            ('svm_clf', svm.LinearSVC(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_w2v = Pipeline([\n",
        "            ('sgd_tfidf', w2v),\n",
        "            ('sgd_clf', svm.LinearSVC(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_glove = Pipeline([\n",
        "            ('sgd_tfidf', glove),\n",
        "            ('sgd_clf', svm.LinearSVC(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_fast = Pipeline([\n",
        "            ('sgd_tfidf', fasttext),\n",
        "            ('sgd_clf', svm.LinearSVC(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        svm_cv.fit(statement, label)\n",
        "        svm_tf.fit(statement, label)\n",
        "        svm_w2v.fit(statement, label)\n",
        "        svm_glove.fit(statement, label)\n",
        "        svm_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = svm_cv\n",
        "        classificadores[\"TFIDF\"] = svm_tf\n",
        "        classificadores[\"Word2Vec\"] = svm_w2v\n",
        "        classificadores[\"Glove\"] = svm_glove\n",
        "        classificadores[\"FastText\"] = svm_fast\n",
        "\n",
        "        return classificadores        \n",
        "\n",
        "    def get_extraTrees(self, statement, label, glove, w2v, fasttext):\n",
        "        cv_ex = CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                                encoding='utf-8', input='content',\n",
        "                                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "                                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "                                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                                tokenizer=None, vocabulary=None)\n",
        "\n",
        "        ex_cv1 = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "                                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                                     min_samples_leaf=1, min_samples_split=2,\n",
        "                                     min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
        "                                     oob_score=False, random_state=self.rng, verbose=0, warm_start=False)\n",
        "\n",
        "        ex_cv2 = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "                                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                                      min_samples_leaf=1, min_samples_split=2,\n",
        "                                      min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
        "                                      oob_score=False, random_state=self.rng, verbose=0, warm_start=False)\n",
        "\n",
        "        ex_cv3 = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "                                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                                      min_samples_leaf=1, min_samples_split=2,\n",
        "                                      min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
        "                                      oob_score=False, random_state=self.rng, verbose=0, warm_start=False)\n",
        "\n",
        "        ex_cv4 = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "                                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                                      min_samples_leaf=1, min_samples_split=2,\n",
        "                                      min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
        "                                      oob_score=False, random_state=self.rng, verbose=0, warm_start=False)\n",
        "\n",
        "        ex_cv5 = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "                                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                                      min_samples_leaf=1, min_samples_split=2,\n",
        "                                      min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=None,\n",
        "                                      oob_score=False, random_state=self.rng, verbose=0, warm_start=False)\n",
        "\n",
        "        extp_cv = Pipeline([\n",
        "            ('NBCV', cv_ex),\n",
        "            ('nb_clf', ex_cv1)])\n",
        "\n",
        "        tf_ex = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
        "                                encoding='utf-8', input='content',\n",
        "                                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "                                ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
        "                                stop_words='english', strip_accents=None, sublinear_tf=False,\n",
        "                                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "                                vocabulary=None)\n",
        "\n",
        "        extp_tf = Pipeline([\n",
        "            ('NBCV', tf_ex),\n",
        "            ('nb_clf', ex_cv2)])\n",
        "\n",
        "        extp_w2 = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('nb_clf', ex_cv3)])\n",
        "\n",
        "        extp_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('nb_clf', ex_cv4)])\n",
        "\n",
        "        extp_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('nb_clf', ex_cv5)])\n",
        "\n",
        "        extp_cv.fit(statement, label)\n",
        "        extp_tf.fit(statement, label)\n",
        "        extp_w2.fit(statement, label)\n",
        "        extp_glove.fit(statement, label)\n",
        "        extp_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = extp_cv\n",
        "        classificadores[\"TFIDF\"] = extp_tf\n",
        "        classificadores[\"Word2Vec\"] = extp_w2\n",
        "        classificadores[\"Glove\"] = extp_glove\n",
        "        classificadores[\"FastText\"] = extp_fast\n",
        "\n",
        "        return classificadores\n",
        "\n",
        "    \n",
        "    def get_rfc(self, statement, label, glove, w2v, fasttext):\n",
        "\n",
        "        cv_rf = CountVectorizer()\n",
        "\n",
        "        rf_c = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        rf_c1 = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        rf_c2 = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        rf_c3 = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        rf_tf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "        rfp_cv = Pipeline([\n",
        "            ('NBCV', cv_rf),\n",
        "            ('nb_clf', rf_c)])\n",
        "\n",
        "        tf_cv = TfidfVectorizer()\n",
        "\n",
        "      \n",
        "        \n",
        "\n",
        "        rfp_tf = Pipeline([\n",
        "            ('NBCV', tf_cv),\n",
        "            ('nb_clf', rf_tf)])\n",
        "\n",
        "        rf_w2v = Pipeline([\n",
        "            ('NBCV', w2v),\n",
        "            ('clf', rf_c1)])\n",
        "\n",
        "        rf_glove = Pipeline([\n",
        "            ('NBCV', glove),\n",
        "            ('clf', rf_c2)])\n",
        "\n",
        "        rf_fast = Pipeline([\n",
        "            ('NBCV', fasttext),\n",
        "            ('clf', rf_c3)])\n",
        "\n",
        "        rfp_cv.fit(statement, label)\n",
        "        rfp_tf.fit(statement, label)\n",
        "        rf_w2v.fit(statement, label)\n",
        "        rf_glove.fit(statement, label)\n",
        "        rf_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = rfp_cv\n",
        "        classificadores[\"TFIDF\"] = rfp_tf\n",
        "        classificadores[\"Word2Vec\"] = rf_w2v\n",
        "        classificadores[\"Glove\"] = rf_glove\n",
        "        classificadores[\"FastText\"] = rf_fast\n",
        "\n",
        "        return classificadores\n",
        "\n",
        "    def get_mlp(self, statement, label, glove, w2v, fasttext):\n",
        "        cv_ex = CountVectorizer()\n",
        "\n",
        "        tf_cv = TfidfVectorizer()\n",
        "\n",
        "        mpl_cv = Pipeline([\n",
        "            ('svmCV', cv_ex),\n",
        "            ('svm_clf', MLPClassifier(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        mpl_tf = Pipeline([\n",
        "            ('svmCV', tf_cv),\n",
        "            ('svm_clf', MLPClassifier(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        mpl_w2v = Pipeline([\n",
        "            ('sgd_tfidf', w2v),\n",
        "            ('sgd_clf', MLPClassifier(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        mpl_glove = Pipeline([\n",
        "            ('sgd_tfidf', glove),\n",
        "            ('sgd_clf', MLPClassifier(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        mpl_fast = Pipeline([\n",
        "            ('sgd_tfidf', fasttext),\n",
        "            ('sgd_clf', MLPClassifier(random_state=self.rng))\n",
        "        ])\n",
        "\n",
        "        mpl_cv.fit(statement, label)\n",
        "        mpl_tf.fit(statement, label)\n",
        "        mpl_w2v.fit(statement, label)\n",
        "        mpl_glove.fit(statement, label)\n",
        "        mpl_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = mpl_cv\n",
        "        classificadores[\"TFIDF\"] = mpl_tf\n",
        "        classificadores[\"Word2Vec\"] = mpl_w2v\n",
        "        classificadores[\"Glove\"] = mpl_glove\n",
        "        classificadores[\"FastText\"] = mpl_fast\n",
        "\n",
        "        return classificadores\n",
        "\n",
        "\n",
        "    def get_multinomial_nb(self, statement, statement_teste, label, glove, w2v, fasttext):\n",
        "\n",
        "\n",
        "        mnb_cv = Pipeline([\n",
        "            ('1', CountVectorizer()),\n",
        "            ('2', MultinomialNB())])\n",
        "\n",
        "        mnb_tf = Pipeline([\n",
        "            ('1', TfidfVectorizer()),\n",
        "            ('2', MultinomialNB())])\n",
        "\n",
        "        mnb_w2v = Pipeline([\n",
        "            ('1', w2v),\n",
        "            ('2', MinMaxScaler(feature_range=(0, 1))),\n",
        "            ('3', MultinomialNB())])\n",
        "\n",
        "        mnb_glove = Pipeline([\n",
        "            ('1', glove),\n",
        "            ('2', MinMaxScaler(feature_range=(0, 1))),\n",
        "            ('3', MultinomialNB())])\n",
        "\n",
        "        mnb_fast = Pipeline([\n",
        "            ('1', fasttext),\n",
        "            ('2', MinMaxScaler(feature_range=(0, 1))),\n",
        "            ('3', MultinomialNB())])\n",
        "\n",
        "        mnb_cv.fit(statement, label)\n",
        "        mnb_tf.fit(statement, label)\n",
        "        mnb_w2v.fit(statement, label)\n",
        "        mnb_glove.fit(statement, label)\n",
        "        mnb_fast.fit(statement, label)\n",
        "\n",
        "        classificadores = {}\n",
        "\n",
        "        classificadores[\"CV\"] = mnb_cv\n",
        "        classificadores[\"TFIDF\"] = mnb_tf\n",
        "        classificadores[\"Word2Vec\"] = mnb_w2v\n",
        "        classificadores[\"Glove\"] = mnb_glove\n",
        "        classificadores[\"FastText\"] = mnb_fast\n",
        "\n",
        "        return classificadores\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a53PB3bwRPnP"
      },
      "source": [
        "df_train, df_valid, df_test, df = mestrado_util.get_dataset()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAzfoBAwRJgw"
      },
      "source": [
        "treinamento = pd.DataFrame(columns=['statement', 'label', 'label_binario'])\n",
        "\n",
        "i = 0\n",
        "for index, row in df_train.iterrows():\n",
        "  label = row['label']\n",
        "  binario = '0'\n",
        "  if label == 'true' or label == 'mostly-true' or label ==  'half-true' :\n",
        "    binario = '1'\n",
        "    \n",
        "  treinamento = treinamento.append({'statement': row['statement'], 'label': label, 'label_binario': binario }, ignore_index=True)\n",
        "  \n",
        "  \n",
        "for index, row in df_valid.iterrows():\n",
        "  label = row['label']\n",
        "  binario = '0'\n",
        "  if label == 'true' or label == 'mostly-true' or label ==  'half-true' :\n",
        "    binario = '1'\n",
        "  \n",
        "  treinamento = treinamento.append({'statement': row['statement'], 'label': label, 'label_binario': binario }, ignore_index=True)\n",
        "  \n",
        "  \n",
        "teste = pd.DataFrame(columns=['statement', 'label', 'label_binario'])\n",
        "df_test['label_binario'] = ' - '\n",
        "i = 0\n",
        "for index, row in df_test.iterrows():\n",
        "  label = row['label']\n",
        "  binario = '0'\n",
        "  if label == 'true' or label == 'mostly-true' or label ==  'half-true' :\n",
        "    binario = '1'\n",
        "  \n",
        "  df_test.at[index,'label_binario']=binario\n",
        "  teste = teste.append({'statement': row['statement'], 'label': label, 'label_binario': binario }, ignore_index=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X7OUYnjLv9j"
      },
      "source": [
        "#cod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltei7r9ALw7U"
      },
      "source": [
        "mestrado_oracle = Thiago_Mestrado_Oracle()\n",
        "mestrado_util = Mestrado_Thiago_util()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6DQosodMSsE",
        "outputId": "f4ee9f76-2ff9-4197-c378-7e4bb86e40d0"
      },
      "source": [
        "w2v = EmbeddingTransformer('word2vec')\n",
        "glove = EmbeddingTransformer('glove')\n",
        "fasttext = EmbeddingTransformer('fasttext')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.7% 1657.1/1662.8MB downloaded\n",
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "[=================================================-] 98.8% 947.1/958.4MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhR4ovl-R3l8"
      },
      "source": [
        "## classificadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H_JLSrER47j"
      },
      "source": [
        "svm_c = mestrado_util.get_svm_linear_svc(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fYJ5B6pR8Ae"
      },
      "source": [
        "mnb = mestrado_util.get_multinomial_nb(treinamento['statement'], teste['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwTArENwR-ak"
      },
      "source": [
        "sgd = mestrado_util.get_sgd(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K0KCwD8SD7Z"
      },
      "source": [
        "pca = mestrado_util.get_pac(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmLpqyeYSGWr"
      },
      "source": [
        "perceptron = mestrado_util.get_perceptron(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uclA0XxiSJJa"
      },
      "source": [
        "ext = mestrado_util.get_extraTrees(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS2ejVlLSLzL"
      },
      "source": [
        "rf = mestrado_util.get_rfc(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc_oU35KSPWK"
      },
      "source": [
        "mlp = mestrado_util.get_mlp(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWtvn1lgSRYn"
      },
      "source": [
        "lr = mestrado_util.get_lr(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPoW7R6MSUIq"
      },
      "source": [
        "knn = mestrado_util.get_knn(treinamento['statement'], treinamento['label_binario'], glove, w2v, fasttext)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr-F3avCSW5d"
      },
      "source": [
        "results = pd.DataFrame(columns=['Técnica', 'CV', 'TFIDF',  'W2V', 'GloVe', 'Fasttext'])\n",
        "combinacao = pd.DataFrame(columns=['Técnica', 'Voto', 'Average'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqid2THWSayM"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}